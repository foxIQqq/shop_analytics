# DAG для Shop Analytics

В данной директории содержатся DAG-файлы (Directed Acyclic Graph) для Apache Airflow, которые используются для оркестрации задач в проекте "Shop Analytics".

## Структура DAG-файлов

### 1. daily_job.py

Ежедневная обработка данных для аналитики. Запускается каждый день в 2:00 утра.

Задачи:
- Выгрузка товаров из Parquet-файла в S3 (RAW слой)
- Выгрузка данных о клиентах из PostgreSQL в S3 (RAW слой)
- Копирование данных из RAW в STAGE слой с валидацией
- Запуск Spark-джобы для обработки данных и создания аналитических агрегатов
- Запуск обработки данных из Kafka для стриминга

### 2. api_data_processing.py

Обработка данных, полученных через API. Запускается каждые 30 минут.

Задачи:
- Проверка наличия новых данных в API
- Обработка данных о покупках из API (передача в Kafka)
- Запуск обработки данных из Kafka и сохранение в S3
- Обновление STAGE слоя

## Интеграция Airflow с Shop Analytics

Airflow используется для автоматизации и оркестрации следующих процессов:

1. **Сбор данных из различных источников**:
   - Клиенты из PostgreSQL
   - Покупки из Kafka
   - Товары из S3/Parquet файлов

2. **Обработка данных**:
   - Проверка и валидация
   - Копирование в Stage слой
   - Выполнение ETL процессов

3. **Создание аналитики**:
   - Запуск Spark-джоб для агрегации данных
   - Формирование отчетов в ClickHouse

## Настройка и запуск

Airflow автоматически инициализируется при запуске системы через `start.sh`. Веб-интерфейс доступен по адресу:
```
http://localhost:8090
```

Учетные данные для входа:
- Логин: admin
- Пароль: admin

## Мониторинг

Airflow предоставляет удобный веб-интерфейс для мониторинга выполнения задач:
- График (Graph View) - показывает зависимости между задачами
- История (Tree View) - историческое представление выполнения
- Логи задач - подробные логи каждой задачи 