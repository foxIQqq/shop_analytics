# Shop Analytics System

Микросервисная система для аналитики продаж в интернет-магазине.

## Архитектура проекта

Система состоит из следующих компонентов:

- **Источники данных**:
  - PostgreSQL — информация о клиентах
  - Kafka — данные о покупках
  - S3 (MinIO) — данные о товарах

- **Слои данных**:
  - Stage слой в S3 — промежуточное хранилище для всех типов данных
  - Аналитический слой — обработанные данные для бизнес-аналитики

- **Обработка данных**:
  - Spark — пакетная обработка данных и потоковая обработка
  - Airflow — оркестрация ETL процессов

- **API сервисы**:
  - REST API для продавцов, товаров и покупок
  - Интеграция с Kafka для обработки событий

- **Визуализация/Аналитика**:
  - ClickHouse для быстрых аналитических запросов

## Структура проекта

```
app/
├── api/                  # REST API сервисы
│   ├── routers/          # Маршрутизаторы API
│   └── utils/            # Утилиты для API
├── batch_processing/     # Модули пакетной обработки данных
├── configs/              # Файлы конфигурации
├── dag/                  # DAG файлы для Airflow
│   └── daily_job.py      # DAG для ежедневных задач
├── data_ingestion/       # Модули для загрузки данных
│   ├── kafka_to_s3.py    # Загрузка данных из Kafka в S3
│   ├── postgres_to_s3.py # Загрузка данных из PostgreSQL в S3
│   └── s3_stage_loader.py # Загрузка данных в stage-слой
├── infrastructure/       # Инфраструктурные файлы
│   ├── docker-compose.yml # Определение сервисов
│   └── init_db.sql       # Скрипт инициализации БД
├── utils/                # Общие утилиты
│   └── s3_client.py      # Клиент для работы с S3
└── start.sh             # Скрипт для запуска системы
```

## Компоненты системы

### Data Ingestion & Stage Layer
- Загрузка данных из PostgreSQL в S3 (MinIO)
- Загрузка данных из Kafka в S3
- Загрузка товаров в Stage слой
- Airflow DAG для оркестрации ежедневных задач

### Batch Processing & Analytics
- Ежедневный Spark Job
- ETL для товаров и клиентов
- Агрегаты для аналитики
- Загрузка результатов в ClickHouse

### API Development & Integration
- REST API для продавцов
- API для товаров и покупок
- Интеграция с Kafka Producers

## Быстрый старт

### Требования
- Docker и Docker Compose

### Запуск системы
```bash
# Клонировать репозиторий
git clone <repository-url>
cd shop_analytics

# Запустить систему
./app/start.sh
```

Скрипт автоматически выполнит следующие шаги:
1. Запустит все необходимые контейнеры
2. Создаст топики Kafka
3. Отправит тестовые сообщения в Kafka
4. Выгрузит данные из PostgreSQL в S3
5. Запустит Spark-стриминг для загрузки данных из Kafka в S3
6. Загрузит products.parquet в S3
7. Обработает данные для Stage-слоя

### Доступные сервисы
- API: http://localhost:8000
- MinIO Console: http://localhost:9001 (логин: minioadmin / пароль: minioadmin)
- Airflow: http://localhost:8081 (логин: admin / пароль: admin)
- Spark UI: http://localhost:8080
- ClickHouse HTTP: http://localhost:8123

## Дополнительная информация

### Принципы обработки данных
- **Потоковая обработка**: данные о покупках из Kafka сразу обрабатываются и загружаются в ClickHouse
- **Пакетная обработка**: раз в сутки обновляются данные о товарах и клиентах

### Расширение проекта
Система легко расширяема. Можно добавить:
- Дополнительные источники данных
- Новые типы аналитики
- Модули машинного обучения для прогнозирования продаж
