services:
  postgres:
    image: postgres:13
    container_name: shop-postgres
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
      POSTGRES_DB: shop
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ../infrastructure/init_db.sql:/docker-entrypoint-initdb.d/init_db.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d shop"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  zookeeper:
    image: confluentinc/cp-zookeeper:7.0.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.0.0
    depends_on:
      - zookeeper
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    ports:
      - "9092:9092"

  minio:
    image: minio/minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data

  clickhouse:
    image: clickhouse/clickhouse-server:23.8
    ports:
      - "8123:8123"
      - "19000:9000"
    environment:
      - CLICKHOUSE_USER=default
      - CLICKHOUSE_PASSWORD=
      - CLICKHOUSE_DB=analytics
      - CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT=1
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ../streaming/clickhouse_setup.sql:/docker-entrypoint-initdb.d/clickhouse_setup.sql
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--spider", "--tries=1", "http://localhost:8123/ping"]
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 10s

  spark-master:
    image: bitnami/spark:3.4
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    ports:
      - "8080:8080"
    volumes:
      - ../data_ingestion:/app/data_ingestion:ro
      - ../utils:/app/utils:ro
      - ../batch_processing:/app/batch_processing:ro
      - ../streaming:/app/streaming:ro
      - ../configs:/app/configs:ro
      - spark_repository:/opt/bitnami/spark/.ivy

  spark-worker:
    image: bitnami/spark:3.4
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - ../data_ingestion:/app/data_ingestion:ro
      - ../utils:/app/utils:ro
      - ../batch_processing:/app/batch_processing:ro
      - ../streaming:/app/streaming:ro
      - ../configs:/app/configs:ro
      - spark_repository:/opt/bitnami/spark/.ivy

  api:
    build:
      context: ..
      dockerfile: api/Dockerfile
    ports:
      - "8000:8000"
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      POSTGRES_URL: postgresql://admin:password@postgres:5432/shop
      MINIO_ENDPOINT: "http://minio:9000"
      MINIO_ACCESS_KEY: "minioadmin"
      MINIO_SECRET_KEY: "minioadmin"
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - ../data_ingestion:/app/data_ingestion
      - ../utils:/app/utils

  init-services:
    image: docker:latest
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ../streaming/clickhouse_setup.sql:/clickhouse_setup.sql
    command: >
      sh -c "
        sleep 15
        
        docker exec kafka bash -c '
          kafka-topics --create --topic purchases --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1 --if-not-exists &&
          kafka-topics --create --topic products-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1 --if-not-exists &&
          kafka-topics --create --topic sellers-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1 --if-not-exists &&
          kafka-topics --create --topic customers-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1 --if-not-exists
        '
        
        docker run --rm --network infrastructure_default minio/mc config host add myminio http://minio:9000 minioadmin &&
        docker run --rm --network infrastructure_default minio/mc mb --ignore-existing myminio/shop-raw-data &&
        docker run --rm --network infrastructure_default minio/mc mb --ignore-existing myminio/shop-stage-data &&
        docker run --rm --network infrastructure_default minio/mc mb --ignore-existing myminio/analytics &&
        docker run --rm --network infrastructure_default minio/mc mb --ignore-existing myminio/analytics/checkpoints/purchase_processor -p
        
        docker exec spark-master bash -c 'pip install minio pandas pyarrow clickhouse-driver python-dotenv'
        
        echo 'Waiting for ClickHouse to start...'
        for i in $(seq 1 30); do
          if curl -s http://clickhouse:8123/ >/dev/null; then
            echo 'ClickHouse is ready!'
            break
          fi
          echo 'Waiting for ClickHouse...'
          sleep 2
        done
        
        echo 'Initializing ClickHouse tables...'
        cat /clickhouse_setup.sql | docker exec -i clickhouse clickhouse-client --multiquery || echo 'Failed to initialize ClickHouse tables'
        
        echo 'Checking ClickHouse tables...'
        docker exec clickhouse clickhouse-client --query 'SHOW DATABASES' || echo 'Failed to query ClickHouse'
        docker exec clickhouse clickhouse-client --query 'SHOW TABLES FROM analytics' || echo 'No tables in analytics database'
        
        echo 'Initialization completed successfully!'
      "
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_started
      minio:
        condition: service_started
      clickhouse:
        condition: service_started
    restart: "no"

  streaming-processor:
    image: bitnami/spark:3.4
    depends_on:
      - spark-master
      - init-services
      - clickhouse
      - kafka
      - minio
    environment:
      - SPARK_MODE=client
      - SPARK_MASTER_URL=spark://spark-master:7077
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_TOPIC=purchases
      - S3_ENDPOINT=http://minio:9000
      - S3_ACCESS_KEY=minioadmin
      - S3_SECRET_KEY=minioadmin
      - ICEBERG_CATALOG_NAME=iceberg
      - CLICKHOUSE_HOST=clickhouse
      - CLICKHOUSE_PORT=9000
      - CLICKHOUSE_USER=default
      - CLICKHOUSE_PASSWORD=
    volumes:
      - ../data_ingestion:/app/data_ingestion:ro
      - ../utils:/app/utils:ro
      - ../batch_processing:/app/batch_processing:ro
      - ../streaming:/app/streaming:ro
      - ../configs:/app/configs:ro
      - spark_repository:/opt/bitnami/spark/.ivy
    command: >
      bash -c "
        sleep 45 &&
        
        apt-get update && apt-get install -y curl python3-pip &&
        
        echo 'Checking if ClickHouse is ready...' &&
        until curl -s http://clickhouse:8123/ >/dev/null; do
          echo 'Waiting for ClickHouse...'
          sleep 2
        done &&
        echo 'ClickHouse is ready!' &&
        
        echo 'Checking if analytics bucket exists...' &&
        pip install minio &&
        
        echo 'from minio import Minio; c=Minio(\"minio:9000\",access_key=\"minioadmin\",secret_key=\"minioadmin\",secure=False); c.make_bucket(\"analytics\") if not c.bucket_exists(\"analytics\") else print(\"Bucket exists\")' > /tmp/check_bucket.py &&
        python3 /tmp/check_bucket.py &&
        
        echo 'Creating necessary subfolders in analytics bucket...' &&
        echo 'from minio import Minio; import io; c=Minio(\"minio:9000\",access_key=\"minioadmin\",secret_key=\"minioadmin\",secure=False); c.put_object(\"analytics\", \"checkpoints/.keep\", io.BytesIO(b\"\"), 0) if not any(obj.object_name.startswith(\"checkpoints/\") for obj in c.list_objects(\"analytics\", prefix=\"checkpoints/\", recursive=False)) else print(\"Checkpoints folder exists\")' > /tmp/create_folders.py &&
        python3 /tmp/create_folders.py &&
        
        export PYTHONPATH=/app &&
        /opt/bitnami/spark/bin/spark-submit 
          --master spark://spark-master:7077 
          --packages org.apache.hadoop:hadoop-aws:3.3.4,org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.3,org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.3,com.clickhouse:clickhouse-jdbc:0.4.6
          --conf 'spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog'
          --conf 'spark.sql.catalog.iceberg.type=hadoop'
          --conf 'spark.sql.catalog.iceberg.warehouse=s3a://analytics/'
          --conf 'spark.hadoop.fs.s3a.endpoint=http://minio:9000'
          --conf 'spark.hadoop.fs.s3a.access.key=minioadmin'
          --conf 'spark.hadoop.fs.s3a.secret.key=minioadmin'
          --conf 'spark.hadoop.fs.s3a.path.style.access=true'
          --conf 'spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'
          /app/streaming/purchase_processor.py
      "

  clickhouse-ui:
    image: spoonest/clickhouse-tabix-web-client
    depends_on:
      clickhouse:
        condition: service_healthy
    environment:
      - CH_HOST=clickhouse
      - CH_PORT=8123
      - CH_USER=default
      - CH_PASSWORD=
      - CH_HTTP_PATH=
      - CLICKHOUSE_HOST=http://clickhouse:8123
      - TABIX_USER=default
      - TABIX_PASSWORD=
      - AUTO_LOGIN=true
      - SERVER_HOST=0.0.0.0
      - TABIX_PORT=80
    ports:
      - "8124:80"
    volumes:
      - ../streaming/tabix-config.json:/var/www/html/tabix-config.json
    restart: unless-stopped

  airflow-postgres:
    image: postgres:13
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow_postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  airflow-webserver:
    build:
      context: .
      dockerfile: airflow-Dockerfile
    container_name: airflow-webserver
    depends_on:
      airflow-postgres:
        condition: service_healthy
      minio:
        condition: service_started
      kafka:
        condition: service_started
      clickhouse:
        condition: service_started
    environment:
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__WEBSERVER__SECRET_KEY: "shop-analytics-secret-key"
      PYTHONPATH: "/opt/airflow"
      PROJECT_HOME: "/opt/airflow"
      MINIO_ENDPOINT: "http://minio:9000"
      MINIO_ROOT_USER: "minioadmin"
      MINIO_ROOT_PASSWORD: "minioadmin"
      KAFKA_BOOTSTRAP_SERVERS: "kafka:9092"
      KAFKA_TOPIC_PURCHASES: "purchases"
      S3_BUCKET_RAW: "shop-raw-data"
      S3_BUCKET_STAGE: "shop-stage-data"
      CLICKHOUSE_HOST: "clickhouse"
      CLICKHOUSE_PORT: "9000"
      CLICKHOUSE_USER: "default"
      CLICKHOUSE_PASSWORD: ""
    volumes:
      - ../dag:/opt/airflow/dags
      - ../data_ingestion:/opt/airflow/data_ingestion
      - ../batch_processing:/opt/airflow/batch_processing
      - ../utils:/opt/airflow/utils
      - airflow_logs:/opt/airflow/logs
    ports:
      - "8090:8080"
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  airflow-scheduler:
    build:
      context: .
      dockerfile: airflow-Dockerfile
    container_name: airflow-scheduler
    depends_on:
      airflow-webserver:
        condition: service_healthy
      minio:
        condition: service_started
      kafka:
        condition: service_started
      clickhouse:
        condition: service_started
    environment:
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      PYTHONPATH: "/opt/airflow"
      PROJECT_HOME: "/opt/airflow"
      MINIO_ENDPOINT: "http://minio:9000"
      MINIO_ROOT_USER: "minioadmin"
      MINIO_ROOT_PASSWORD: "minioadmin"
      KAFKA_BOOTSTRAP_SERVERS: "kafka:9092"
      KAFKA_TOPIC_PURCHASES: "purchases"
      S3_BUCKET_RAW: "shop-raw-data"
      S3_BUCKET_STAGE: "shop-stage-data"
      CLICKHOUSE_HOST: "clickhouse"
      CLICKHOUSE_PORT: "9000"
      CLICKHOUSE_USER: "default"
      CLICKHOUSE_PASSWORD: ""
    volumes:
      - ../dag:/opt/airflow/dags
      - ../data_ingestion:/opt/airflow/data_ingestion
      - ../batch_processing:/opt/airflow/batch_processing
      - ../utils:/opt/airflow/utils
      - airflow_logs:/opt/airflow/logs
    command: scheduler

  airflow-init:
    build:
      context: .
      dockerfile: airflow-Dockerfile
    container_name: airflow-init
    depends_on:
      airflow-postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    volumes:
      - ../dag:/opt/airflow/dags
      - ../data_ingestion:/opt/airflow/data_ingestion
      - ../batch_processing:/opt/airflow/batch_processing
      - ../utils:/opt/airflow/utils
      - airflow_logs:/opt/airflow/logs
    entrypoint: /bin/bash
    command: -c "airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com"
    restart: "no"

volumes:
  postgres_data:
  minio_data:
  clickhouse_data:
  spark_repository:
  airflow_postgres_data:
  airflow_logs:

networks:
  default:
    driver: bridge