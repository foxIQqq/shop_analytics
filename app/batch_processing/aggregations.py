from pyspark.sql.functions import sum, count, max, min, col, datediff, lit
from datetime import datetime

def run_aggregations(spark):
    # –ß—Ç–µ–Ω–∏–µ –∏–∑ Iceberg —Ç–∞–±–ª–∏—Ü –≤–º–µ—Å—Ç–æ –ø—Ä—è–º–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ —Ñ–∞–π–ª–∞–º
    purchases = spark.table("analytics.purchases")
    products = spark.table("analytics.products")

    # –ü—Ä–∏—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    df = purchases.join(products, on="product_id")

    # üí° 1. –ü—Ä–æ–¥–∞–∂–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    category_sales = df.groupBy("category").agg(
        sum("price_at_time").alias("total_revenue"),
        count("*").alias("total_sales")
    )

    category_sales.write.format("jdbc").options(
        url="jdbc:clickhouse://clickhouse:8123/default",
        driver="com.clickhouse.jdbc.ClickHouseDriver",
        dbtable="category_sales",
        user="default",
        password=""
    ).mode("overwrite").save()

    # üí° 2. RFM-–∞–Ω–∞–ª–∏–∑
    current_date = datetime.today().strftime('%Y-%m-%d')
    rfm = df.groupBy("customer_id").agg(
        max("purchased_at").alias("last_purchase"),
        count("*").alias("frequency"),
        sum("price_at_time").alias("monetary")
    ).withColumn("recency", datediff(lit(current_date), col("last_purchase")))

    rfm.write.format("jdbc").options(
        url="jdbc:clickhouse://clickhouse:8123/default",
        driver="com.clickhouse.jdbc.ClickHouseDriver",
        dbtable="customer_rfm",
        user="default",
        password=""
    ).mode("overwrite").save()

    # üí° 3. –¢–æ–ø —Ç–æ–≤–∞—Ä–æ–≤
    top_products = df.groupBy("product_id").agg(
        count("*").alias("purchase_count"),
        sum("price_at_time").alias("revenue")
    ).orderBy(col("revenue").desc())

    top_products.write.format("jdbc").options(
        url="jdbc:clickhouse://clickhouse:8123/default",
        driver="com.clickhouse.jdbc.ClickHouseDriver",
        dbtable="top_products",
        user="default",
        password=""
    ).mode("overwrite").save()